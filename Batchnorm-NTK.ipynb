{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does an infinitely wide neural network with batchnorm look like? We will calculate the [Neural Tangent Kernel](https://arxiv.org/abs/1806.07572) of a batchnorm-ReLU Multi-Layer Perceptron in this notebook, according to [our paper](https://arxiv.org/abs/2006.14548), so you can find out yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install quadpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:16.495319Z",
     "start_time": "2020-06-17T21:12:14.955041Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import quadpy as qp\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:16.499714Z",
     "start_time": "2020-06-17T21:12:16.496889Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import Gmatrix, J1, VReLU, VStep, thVReLU, clone_grads, paramdot, flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider batchnorm with $B$ batch elements.\n",
    "Let $x_1, \\ldots, x_B \\in \\mathbb{R}^d$ be a batch of inputs.\n",
    "Let $\\widetilde{\\mathrm{relu}}: \\mathbb R^B \\to \\mathbb R^B$ denote batchnorm followed by ReLU.\n",
    "\n",
    "$$\\widetilde{\\mathrm{relu}}(z) = \\mathrm{relu}\\left(\\frac{z - \\mu}{\\sigma}\\right),\\quad\n",
    "\\text{where $\\mu = \\frac 1 B \\sum_{i=1}^B z_i$ and $\\sigma^2 = \\frac 1 B \\sum_{i=1}^B (z_i - \\mu)^2$}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/batchnormMLP.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Finite-Width Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:16.520060Z",
     "start_time": "2020-06-17T21:12:16.501553Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, nonlin=nn.ReLU, width=500, depth=2, inputdim=None, eps=0, initstyle='standard'):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.nonlin = nonlin\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.params = nn.ParameterList()\n",
    "        self.BNs = nn.ModuleList()\n",
    "        self.nonlins = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                if inputdim is None:\n",
    "                    inputdim = width\n",
    "                if initstyle == 'standard':\n",
    "                    w = nn.Parameter(torch.randn(width, inputdim) / np.sqrt(inputdim))\n",
    "                                 \n",
    "                else:\n",
    "                    w = nn.Parameter(torch.randn(width, inputdim))\n",
    "            else:\n",
    "                if initstyle == 'standard':\n",
    "                    w = nn.Parameter(torch.randn(width, width) / np.sqrt(width))\n",
    "                else:\n",
    "                    w = nn.Parameter(torch.randn(width, width))\n",
    "            self.params.append(w)\n",
    "            self.BNs.append(nn.BatchNorm1d(width, eps=eps, momentum=None, affine=False, track_running_stats=False))\n",
    "            self.nonlins.append(nonlin())\n",
    "        self.readout = nn.Parameter(torch.randn(width))\n",
    "        self.xs = []\n",
    "        self.hs = []\n",
    "    def forward(self, x):\n",
    "        for i in range(self.depth):\n",
    "            w = self.params[i]\n",
    "            h = torch.einsum('ij,kj->ki', w, x)\n",
    "            h.retain_grad()\n",
    "            x = self.nonlins[i](self.BNs[i](h))\n",
    "            self.hs.append(h)\n",
    "            self.xs.append(x)\n",
    "        return x @ self.readout / np.sqrt(self.width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Infinite-Width Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We first detail how to propagate the covariances of activations and of gradients before describing how we can combine them to compute the NTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Single Batch Forward Propagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/BN_forward_singlebatch.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We evaluate this integral by 1) reparametrizing exponentially and 2) eigendecomposition to be more efficient. See [this notebook for explanation](https://github.com/thegregyang/GP4A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:16.543575Z",
     "start_time": "2020-06-17T21:12:16.521631Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def VBNReLUIntegrandExp(Sigma):\n",
    "    '''Returns the integrand function of VBNRelU(Sigma), reparametrized exponentially\n",
    "    '''\n",
    "    B = Sigma.shape[0]\n",
    "    G = Gmatrix(B)\n",
    "    eigvals, eigvecs = np.linalg.eigh(G @ Sigma @ G)\n",
    "    # remove the 1st (smallest) eigenvalue, which is 0\n",
    "    logeigvals = np.log(eigvals[1:])\n",
    "    eigvecs = eigvecs[:, 1:]\n",
    "    def f(logs, logmultfactor=0, eps=1e-10):\n",
    "        # Ueigvals.shape = [..., B]\n",
    "        logUeigvals = np.logaddexp(0, np.log(2) + logs[..., None] + logeigvals)\n",
    "        # inteigvals.shape = [..., B]\n",
    "        loginteigvals = logeigvals - logUeigvals\n",
    "        # divide by sqrt(det)\n",
    "        loginteigvals -= 0.5 * np.sum(logUeigvals, axis=-1, keepdims=True)\n",
    "        loginteigvals += logmultfactor\n",
    "        inteigvals = np.exp(loginteigvals)\n",
    "        return VReLU(np.einsum('ij,...j,jk->...ik', eigvecs, inteigvals, eigvecs.T), eps)\n",
    "    return f\n",
    "def VBNReLU(Sigma, npos=10, nneg=10, alpha=1/8):\n",
    "    '''Computes BN+ReLU kernel for a single batch.\n",
    "    Inputs:\n",
    "        Sigma: input kernel (shape = batchsize x batchsize)\n",
    "        npos: number of points for integrating the big s side of the VBNReLU integral\n",
    "        nneg: number of points for integrating the small s side of the VBNReLU integral\n",
    "        alpha: reparametrize the integral by s = exp(alpha r)\n",
    "    Outputs:\n",
    "        VBNReLU(Sigma)\n",
    "    '''\n",
    "    schemepos = qp.e1r.gauss_laguerre(npos, alpha=0)\n",
    "    schemeneg = qp.e1r.gauss_laguerre(nneg, alpha=0)\n",
    "#     intargmax = VBNReLUIntegrandExpArgmax(Sigma).root\n",
    "    intargmax = 0\n",
    "    dim = Sigma.shape[0]\n",
    "    f = VBNReLUIntegrandExp(Sigma)\n",
    "    integrandpos = lambda xs: np.rollaxis(\n",
    "        np.rollaxis(\n",
    "            alpha * f(intargmax + alpha * xs,\n",
    "                logmultfactor=(intargmax + (1 + alpha) * xs)[..., None]),\n",
    "            2, 0),\n",
    "        3, 1)\n",
    "    integrandneg = lambda xs: np.rollaxis(\n",
    "        np.exp(intargmax) * np.rollaxis(f(intargmax - xs),\n",
    "            2, 0),\n",
    "        3, 1)\n",
    "    return dim * (\n",
    "        schemepos.integrate(integrandpos) + schemeneg.integrate(integrandneg)).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Single Batch Backward Propagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/BN_backward_singlebatch1.png\" width=600 />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/BN_backward_singlebatch2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:16.581303Z",
     "start_time": "2020-06-17T21:12:16.544959Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def VBNReLUBackIntegrandExp(Sigma, Delta):\n",
    "    '''Returns the integrand function of VBNRelUBack(Sigma, Delta), reparametrized exponentially\n",
    "    '''\n",
    "    B = Sigma.shape[0]\n",
    "    G = Gmatrix(B)\n",
    "    eigvals, eigvecs = np.linalg.eigh(G @ Sigma @ G)\n",
    "    # remove the 1st (smallest) eigenvalue, which is 0\n",
    "    logeigvals = np.log(eigvals[1:])\n",
    "    eigvecs = eigvecs[:, 1:]\n",
    "    def f(logs, logmultfactor=0, eps=1e-10):\n",
    "        # Ueigvals.shape = [..., B]\n",
    "        logUeigvals = np.logaddexp(0, np.log(2) + logs[..., None] + logeigvals)\n",
    "        # inteigvals.shape = [..., B]\n",
    "        loginteigvals = logeigvals - logUeigvals\n",
    "        inteigvals = np.exp(loginteigvals)\n",
    "        K = np.einsum('ij,...j,jk->...ik', eigvecs, inteigvals, eigvecs.T)\n",
    "        \n",
    "        thK = torch.from_numpy(K)\n",
    "        thK.requires_grad = True\n",
    "        thReluK = thVReLU(thK, eps)\n",
    "        reluK = thReluK.data.numpy()\n",
    "        stepK = VStep(K)\n",
    "        \n",
    "        DeltaNewAx = torch.from_numpy(Delta)\n",
    "        DeltaNewAx = torch.broadcast_tensors(DeltaNewAx, thReluK)[0]\n",
    "        thReluK.backward(DeltaNewAx)\n",
    "        # shape [..., B, B]\n",
    "        J = thK.grad.numpy()\n",
    "        \n",
    "        Lambda1 = Delta * stepK\n",
    "        \n",
    "        # TODO possible speedup if reuse KJ to compute KJK\n",
    "        Lambda2 = 0.5 * np.exp(2 * logs)[..., None, None] * (\n",
    "            K * np.einsum('ij,...ij->...', Delta, reluK)[..., None, None]\n",
    "            + 2 * np.einsum('...ij,...jk,...kl->...il', K, J, K))\n",
    "        \n",
    "        Lambda3 = np.exp(logs)[..., None, None] * \\\n",
    "            np.einsum('...ij,...jk->...ik', K, J)\n",
    "        Lambda3 = Lambda3 + np.einsum('...ij->...ji', Lambda3)\n",
    "        \n",
    "        Lambda = Lambda1 + Lambda2 - Lambda3\n",
    "\n",
    "        # explicitly multiply by B and conjugate by G outside this fun\n",
    "        integrand = Lambda * np.exp(\n",
    "            # divide by sqrt(det)\n",
    "            logmultfactor - 0.5 * np.sum(logUeigvals, axis=-1, keepdims=True)\n",
    "        )[..., None]\n",
    "        \n",
    "        return integrand\n",
    "    return f\n",
    "\n",
    "def VBNReLUBack(Sigma, Delta, npos=10, nneg=10, alpha=1/4):\n",
    "    '''Computes Jacobian(BN+ReLU) kernel for a single batch.\n",
    "    Inputs:\n",
    "        Sigma: input kernel (shape = batchsize x batchsize)\n",
    "        Delta: gradient kernel (shape = batchsize x batchsize)\n",
    "        npos: number of points for integrating the big s side of the VBNReLU integral\n",
    "        nneg: number of points for integrating the small s side of the VBNReLU integral\n",
    "        alpha: reparametrize the integral by s = exp(alpha r)\n",
    "    Outputs:\n",
    "        VBNReLUBack(Sigma){Delta}\n",
    "    '''\n",
    "    schemepos = qp.e1r.gauss_laguerre(npos, alpha=0)\n",
    "    schemeneg = qp.e1r.gauss_laguerre(nneg, alpha=0)\n",
    "    intargmax = 0\n",
    "    dim = Sigma.shape[0]\n",
    "    f = VBNReLUBackIntegrandExp(Sigma, Delta)\n",
    "    integrandpos = lambda xs: np.rollaxis(\n",
    "        np.rollaxis(\n",
    "            alpha * f(intargmax + alpha * xs,\n",
    "                logmultfactor=(intargmax + (1 + alpha) * xs)[..., None]),\n",
    "            2, 0),\n",
    "        3, 1)\n",
    "    integrandneg = lambda xs: np.rollaxis(\n",
    "        np.exp(intargmax) * np.rollaxis(f(intargmax - xs),\n",
    "            2, 0),\n",
    "        3, 1)\n",
    "    out = dim * (\n",
    "        schemepos.integrate(integrandpos) + schemeneg.integrate(integrandneg)).squeeze(-1)\n",
    "    G = Gmatrix(dim)\n",
    "    return G @ out @ G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Cross Batch Forward Propagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/BN_forward_crossbatch.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Again, we evaluate this integral by 1) reparametrizing exponentially and 2) eigendecomposition to be more efficient. See [this notebook for explanation](https://github.com/thegregyang/GP4A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:16.898570Z",
     "start_time": "2020-06-17T21:12:16.582664Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def VBNReLUCrossBatchIntegrand(Xi, Sigma1, Sigma2):\n",
    "    '''Computes the off diagonal block of the BN+ReLU kernel over 2 batches\n",
    "    Input:\n",
    "        Xi: covariance between batch1 and batch2\n",
    "        Sigma1: autocovariance of batch1\n",
    "        Sigma2: autocovariance of batch2\n",
    "    Output:\n",
    "        f: integrand function in the integral for computing cross batch VBNReLU \n",
    "    '''\n",
    "    n1 = Sigma1.shape[0]\n",
    "    n2 = Sigma2.shape[0]\n",
    "    G1 = Gmatrix(n1)\n",
    "    G2 = Gmatrix(n2)\n",
    "    Delta1, A1 = np.linalg.eigh(G1 @ Sigma1 @ G1)\n",
    "    Delta2, A2 = np.linalg.eigh(G2 @ Sigma2 @ G2)\n",
    "    # kill first 0 eigenval\n",
    "    Delta1 = Delta1[1:]\n",
    "    Delta2 = Delta2[1:]\n",
    "    A1 = A1[:, 1:]\n",
    "    A2 = A2[:, 1:]\n",
    "    \n",
    "    Xidot = A1.T @ Xi @ A2\n",
    "    Omegadot = np.block([[np.diag(Delta1), Xidot], [Xidot.T, np.diag(Delta2)]])\n",
    "    Omegadotinv = np.linalg.inv(Omegadot)\n",
    "    \n",
    "    def f(s, t, multfactor=1):\n",
    "        # Ddot.shape = (..., n1+n2-2, n1+n2-2)\n",
    "        Ddot = s[..., None, None] * np.eye(n1-1+n2-1)\n",
    "        Ddot[..., np.arange(n1-1, n1+n2-2), np.arange(n1-1, n1+n2-2)] = t[..., None]\n",
    "        \n",
    "        ## Compute off-diagonal block of VReLU(Pi)\n",
    "        Pitilde = Omegadotinv + 2 * Ddot\n",
    "        Pitilde = np.linalg.inv(Pitilde)\n",
    "        Pi11diag = np.einsum('ij,...jk,ki->...i',\n",
    "                            A1,\n",
    "                            Pitilde[..., :n1-1, :n1-1],\n",
    "                            A1.T)\n",
    "        Pi22diag = np.einsum('ij,...jk,ki->...i',\n",
    "                            A2,\n",
    "                            Pitilde[..., n1-1:, n1-1:],\n",
    "                            A2.T)\n",
    "        Pi12 = np.einsum('ij,...jk,kl->...il',\n",
    "                        A1,\n",
    "                        Pitilde[..., :n1-1, n1-1:],\n",
    "                        A2.T)\n",
    "        C = J1(np.einsum('...i,...ij,...j->...ij',\n",
    "                        Pi11diag**-0.5,\n",
    "                        Pi12,\n",
    "                        Pi22diag**-0.5))\n",
    "        VReLUPi12 = 0.5 * np.einsum('...i,...ij,...j->...ij',\n",
    "                                Pi11diag**0.5,\n",
    "                                C,\n",
    "                                Pi22diag**0.5)\n",
    "        \n",
    "        ## Compute determinant\n",
    "        ind = np.arange(n1+n2-2)\n",
    "        # Ddot <- matrix inverse of Ddot\n",
    "        Ddot[..., ind, ind] = Ddot[..., ind, ind]**-1\n",
    "        logdet = np.linalg.slogdet(Ddot + 2 * Omegadot)[1]\n",
    "        return np.exp(\n",
    "                    (np.log(multfactor)\n",
    "                      + (-n1/2) * np.log(s)\n",
    "                      + (-n2/2) * np.log(t)\n",
    "                      - 1/2 * logdet)[..., None, None] + np.log(VReLUPi12))\n",
    "    return f\n",
    "\n",
    "def VBNReLUCrossBatch(Xi, Sigma1, Sigma2, npos=10, nneg=5,\n",
    "                      alphapos1=1/3, alphaneg1=1,\n",
    "                     alphapos2=1/3, alphaneg2=1):\n",
    "    '''Compute VBNReLU for two batches.\n",
    "    \n",
    "    Inputs:\n",
    "        Xi: covariance between batch1 and batch2\n",
    "        Sigma1: autocovariance of batch1\n",
    "        Sigma2: autocovariance of batch2\n",
    "        npos: number of points for integrating the big s side of the VBNReLU integral\n",
    "            (effective for both dimensions of integration)\n",
    "        nneg: number of points for integrating the small s side of the VBNReLU integral\n",
    "            (effective for both dimensions of integration)\n",
    "        alphapos1: reparametrize the large s integral by s = exp(alpha r) in the 1st dimension\n",
    "        alphaneg1: reparametrize the small s integral by s = exp(alpha r) in the 1st dimension\n",
    "        alphapos2: reparametrize the large s integral by s = exp(alpha r) in the 2nd dimension\n",
    "        alphaneg2: reparametrize the small s integral by s = exp(alpha r) in the 2nd dimension\n",
    "            By tuning the `alpha` parameters, the integrand is closer to being well-approximated by \n",
    "            low-degree Laguerre polynomials, which makes the quadrature more accurate in approximating the integral.\n",
    "    Outputs:\n",
    "        The (batch1, batch2) block of block matrix obtained by\n",
    "        applying VBNReLU^{\\oplus 2} to the kernel of batch1 and batch2\n",
    "    '''\n",
    "    # We will do the integration explicitly ourselves:\n",
    "    #     We obtain sample points and weights via `quadpy`'s Gauss Laguerre quadrature\n",
    "    #     and do the sum ourselves\n",
    "    schemepos = qp.e1r.gauss_laguerre(npos, alpha=0)\n",
    "    schemeneg = qp.e1r.gauss_laguerre(nneg, alpha=0)\n",
    "    dim1 = Sigma1.shape[0]\n",
    "    dim2 = Sigma2.shape[0]\n",
    "    intargmax = (-np.log(2*(dim1-1)), -np.log(2*(dim2-1)))\n",
    "    f = VBNReLUCrossBatchIntegrand(Xi, Sigma1, Sigma2)\n",
    "    # Get the points manually for each dimension\n",
    "    scheme1dpoints = np.concatenate([schemepos.points, -schemeneg.points])\n",
    "    # Get the weights manually for each dimension\n",
    "    scheme1dwts = np.concatenate([schemepos.weights, schemeneg.weights])\n",
    "    # Obtain the points for the whole 2d integration\n",
    "    scheme2dpoints = np.meshgrid(scheme1dpoints, scheme1dpoints)\n",
    "    # Obtain the weights for the whole 2d integration\n",
    "    scheme2dwts = scheme1dwts[:, None] * scheme1dwts[None, :]\n",
    "\n",
    "    def applyalpha(x, alphapos, alphaneg):\n",
    "        xx = np.copy(x)\n",
    "        xx[xx > 0] *= alphapos\n",
    "        xx[xx <= 0] *= alphaneg\n",
    "        return xx\n",
    "    def alphafactor(x, y):\n",
    "        a = np.zeros_like(x)\n",
    "        a[(x > 0) & (y > 0)] = alphapos1 * alphapos2\n",
    "        a[(x > 0) & (y <= 0)] = alphapos1 * alphaneg2\n",
    "        a[(x <= 0) & (y > 0)] = alphaneg1 * alphapos2\n",
    "        a[(x <= 0) & (y <= 0)] = alphaneg1 * alphaneg2\n",
    "        return a\n",
    "        \n",
    "    integrand = lambda inp: \\\n",
    "        f(np.exp(applyalpha(inp[0], alphapos1, alphaneg1) + intargmax[0]), \n",
    "          np.exp(applyalpha(inp[1], alphapos2, alphaneg2) + intargmax[1]),\n",
    "          multfactor=alphafactor(inp[0], inp[1])\n",
    "              * np.pi**-1\n",
    "              * np.exp(applyalpha(inp[0], alphapos1, alphaneg1) + intargmax[0]\n",
    "                     + applyalpha(inp[1], alphapos2, alphaneg2) + intargmax[1]\n",
    "                     + np.abs(inp[0]) + np.abs(inp[1])\n",
    "                      )\n",
    "         )\n",
    "\n",
    "    return np.sqrt(dim1 * dim2) * np.einsum('ij...,ij->...',\n",
    "              integrand(scheme2dpoints),\n",
    "              scheme2dwts\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Cross Batch Backward Propagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/BN_backward_crossbatch.png\" width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:17.069239Z",
     "start_time": "2020-06-17T21:12:16.900077Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def VBNReLUBackCrossBatchIntegrand(Xi, Sigma1, Sigma2, Chi):\n",
    "    '''Computes the off diagonal block of the Jacobian(BN+ReLU) kernel over 2 batches\n",
    "    Input:\n",
    "        Xi: covariance between batch1 and batch2\n",
    "        Sigma1: autocovariance of batch1\n",
    "        Sigma2: autocovariance of batch2\n",
    "        Chi: covariance of gradients btw batch1 and batch2\n",
    "    Output:\n",
    "        f: integrand function in the integral for computing cross batch VBNReLUBack\n",
    "    '''\n",
    "    n1 = Sigma1.shape[0]\n",
    "    n2 = Sigma2.shape[0]\n",
    "    G1 = Gmatrix(n1)\n",
    "    G2 = Gmatrix(n2)\n",
    "    Delta1, A1 = np.linalg.eigh(G1 @ Sigma1 @ G1)\n",
    "    Delta2, A2 = np.linalg.eigh(G2 @ Sigma2 @ G2)\n",
    "    # kill first 0 eigenval\n",
    "    Delta1 = Delta1[1:]\n",
    "    Delta2 = Delta2[1:]\n",
    "    A1 = A1[:, 1:]\n",
    "    A2 = A2[:, 1:]\n",
    "    A = np.block([[A1, np.zeros([n1, n2-1])], [np.zeros([n2, n1-1]), A2]])\n",
    "    ChiSym = np.block([[np.zeros([n1, n1]), Chi],\n",
    "                       [Chi.T, np.zeros([n2, n2])]])\n",
    "    \n",
    "    Xidot = A1.T @ Xi @ A2\n",
    "    Omegadot = np.block([[np.diag(Delta1), Xidot], [Xidot.T, np.diag(Delta2)]])\n",
    "    Omegadotinv = np.linalg.inv(Omegadot)\n",
    "    \n",
    "    def f(s, t, multfactor=1):\n",
    "        ss = s[..., None, None]\n",
    "        tt = t[..., None, None]\n",
    "        # Ddot.shape = (..., n1+n2-2, n1+n2-2)\n",
    "        Ddot = ss * np.eye(n1-1+n2-1)\n",
    "        Ddot[..., np.arange(n1-1, n1+n2-2), np.arange(n1-1, n1+n2-2)] = t[..., None]\n",
    "        \n",
    "        ## Compute off-diagonal block of VReLU(Pi)\n",
    "        Pitilde = Omegadotinv + 2 * Ddot\n",
    "        Pitilde = np.linalg.inv(Pitilde)\n",
    "        Pi = np.einsum('ij,...jk,kl->...il', A, Pitilde, A.T)\n",
    "        \n",
    "        stepPi = VStep(Pi)\n",
    "        thPi = torch.from_numpy(Pi)\n",
    "        thPi.requires_grad = True\n",
    "        thReluPi = thVReLU(thPi)\n",
    "        # shape (..., n1+n2, n1+n2)\n",
    "        reluPi = thReluPi.detach().numpy()\n",
    "        thChiSym = torch.from_numpy(ChiSym)\n",
    "        thChiSym = torch.broadcast_tensors(thChiSym, thReluPi)\n",
    "        thReluPi.backward(thChiSym)\n",
    "        # shape (..., n1+n2, n1+n2)\n",
    "        J = thPi.grad.numpy()\n",
    "\n",
    "        Gamma1 = Chi * stepPi[..., :n1, n1:]\n",
    "        \n",
    "        # < reluPi, ChiSym > * Pi\n",
    "        Gamma2a = 2 * np.einsum(\n",
    "            '...ij,ij,...kl->...kl',\n",
    "            reluPi[..., :n1, n1:], Chi, Pi[..., :n1, n1:]\n",
    "        )\n",
    "        Gamma2b = 2 * np.einsum(\n",
    "            '...ij,...jk,...kl->...il',\n",
    "            Pi, J, Pi\n",
    "        )\n",
    "        Gamma2b = Gamma2b[..., :n1, n1:]\n",
    "        Gamma2 = 2 * ss * tt * (Gamma2a + Gamma2b)\n",
    "        \n",
    "        # t * J * Pi\n",
    "        JPi = np.einsum(\n",
    "            '...ij,...jk->...ik',\n",
    "            J, Pi\n",
    "        )\n",
    "        Gamma3 = tt * JPi[..., :n1, n1:] + \\\n",
    "                ss * np.einsum(\n",
    "                    '...ij->...ji',\n",
    "                    JPi[..., n1:, :n1]\n",
    "                )\n",
    "        Gamma3 *= 2\n",
    "        \n",
    "        Gamma = Gamma1 + Gamma2 - Gamma3\n",
    "        \n",
    "        ## Compute determinant\n",
    "        ind = np.arange(n1+n2-2)\n",
    "        # Ddot <- matrix inverse of Ddot\n",
    "        Ddot[..., ind, ind] = Ddot[..., ind, ind]**-1\n",
    "        logdet = np.linalg.slogdet(Ddot + 2 * Omegadot)[1]\n",
    "        return np.exp(\n",
    "                    (np.log(multfactor)\n",
    "                      + (-n1/2) * np.log(s)\n",
    "                      + (-n2/2) * np.log(t)\n",
    "                      - 1/2 * logdet)[..., None, None]) \\\n",
    "            * Gamma\n",
    "    return f\n",
    "\n",
    "def VBNReLUBackCrossBatch(Xi, Sigma1, Sigma2, Chi, npos=10, nneg=5,\n",
    "                      alphapos1=1/3, alphaneg1=1,\n",
    "                     alphapos2=1/3, alphaneg2=1):\n",
    "    '''Compute the Jacobian(BN+ReLU) kernel for covariance btw two batches.\n",
    "    \n",
    "    Inputs:\n",
    "        Xi: covariance between batch1 and batch2\n",
    "        Sigma1: autocovariance of batch1\n",
    "        Sigma2: autocovariance of batch2\n",
    "        Chi: covariance of gradients btw batch1 and batch2\n",
    "        npos: number of points for integrating the big s side of the VBNReLU integral\n",
    "            (effective for both dimensions of integration)\n",
    "        nneg: number of points for integrating the small s side of the VBNReLU integral\n",
    "            (effective for both dimensions of integration)\n",
    "        alphapos1: reparametrize the large s integral by s = exp(alpha r) in the 1st dimension\n",
    "        alphaneg1: reparametrize the small s integral by s = exp(alpha r) in the 1st dimension\n",
    "        alphapos2: reparametrize the large s integral by s = exp(alpha r) in the 2nd dimension\n",
    "        alphaneg2: reparametrize the small s integral by s = exp(alpha r) in the 2nd dimension\n",
    "            By tuning the `alpha` parameters, the integrand is closer to being well-approximated by \n",
    "            low-degree Laguerre polynomials, which makes the quadrature more accurate in approximating the integral.\n",
    "    Outputs:\n",
    "        array of shape (batchsize1, batchsize2) that describes\n",
    "        the covariance of the gradients btw batch1 and batch2\n",
    "        after backprop through BN+ReLU\n",
    "    '''\n",
    "    # We will do the integration explicitly ourselves:\n",
    "    #     We obtain sample points and weights via `quadpy`'s Gauss Laguerre quadrature\n",
    "    #     and do the sum ourselves\n",
    "    schemepos = qp.e1r.gauss_laguerre(npos, alpha=0)\n",
    "    schemeneg = qp.e1r.gauss_laguerre(nneg, alpha=0)\n",
    "    dim1 = Sigma1.shape[0]\n",
    "    dim2 = Sigma2.shape[0]\n",
    "    intargmax = (-np.log(2*(dim1-1)), -np.log(2*(dim2-1)))\n",
    "    f = VBNReLUBackCrossBatchIntegrand(Xi, Sigma1, Sigma2, Chi)\n",
    "    # Get the points manually for each dimension\n",
    "    scheme1dpoints = np.concatenate([schemepos.points, -schemeneg.points])\n",
    "    # Get the weights manually for each dimension\n",
    "    scheme1dwts = np.concatenate([schemepos.weights, schemeneg.weights])\n",
    "    # Obtain the points for the whole 2d integration\n",
    "    scheme2dpoints = np.meshgrid(scheme1dpoints, scheme1dpoints)\n",
    "    # Obtain the weights for the whole 2d integration\n",
    "    scheme2dwts = scheme1dwts[:, None] * scheme1dwts[None, :]\n",
    "\n",
    "    def applyalpha(x, alphapos, alphaneg):\n",
    "        xx = np.copy(x)\n",
    "        xx[xx > 0] *= alphapos\n",
    "        xx[xx <= 0] *= alphaneg\n",
    "        return xx\n",
    "    def alphafactor(x, y):\n",
    "        a = np.zeros_like(x)\n",
    "        a[(x > 0) & (y > 0)] = alphapos1 * alphapos2\n",
    "        a[(x > 0) & (y <= 0)] = alphapos1 * alphaneg2\n",
    "        a[(x <= 0) & (y > 0)] = alphaneg1 * alphapos2\n",
    "        a[(x <= 0) & (y <= 0)] = alphaneg1 * alphaneg2\n",
    "        return a\n",
    "        \n",
    "    integrand = lambda inp: \\\n",
    "        f(np.exp(applyalpha(inp[0], alphapos1, alphaneg1) + intargmax[0]), \n",
    "          np.exp(applyalpha(inp[1], alphapos2, alphaneg2) + intargmax[1]),\n",
    "          multfactor=alphafactor(inp[0], inp[1])\n",
    "              * np.pi**-1\n",
    "              * np.exp(applyalpha(inp[0], alphapos1, alphaneg1) + intargmax[0]\n",
    "                     + applyalpha(inp[1], alphapos2, alphaneg2) + intargmax[1]\n",
    "                     + np.abs(inp[0]) + np.abs(inp[1])\n",
    "                      )\n",
    "         )\n",
    "\n",
    "    out = np.sqrt(dim1 * dim2) * np.einsum('ij...,ij->...',\n",
    "              integrand(scheme2dpoints),\n",
    "              scheme2dwts\n",
    "             )\n",
    "    G1 = Gmatrix(dim1)\n",
    "    G2 = Gmatrix(dim2)\n",
    "    return G1 @ out @ G2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## NTK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/BN_NTK_singlebatch.png\" width=600 />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/BN_NTK_crossbatch.png\" width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:17.222758Z",
     "start_time": "2020-06-17T21:12:17.071302Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def theorykernels(mykerx, myker1, myker2, depth):\n",
    "    n1 = myker1.shape[0]\n",
    "    n2 = myker2.shape[0]\n",
    "    \n",
    "    ker1s = [myker1]\n",
    "    ker2s = [myker2]\n",
    "    kerxs = [mykerx]\n",
    "    for i in range(depth):\n",
    "        kerxs.append(VBNReLUCrossBatch(kerxs[-1], ker1s[-1], ker2s[-1]))\n",
    "        ker1s.append(VBNReLU(ker1s[-1]))\n",
    "        ker2s.append(VBNReLU(ker2s[-1]))\n",
    "        \n",
    "    def make_single_batch_ntk(kers):\n",
    "        n = kers[0].shape[0]\n",
    "        ntk = np.zeros([n, n])\n",
    "        for i in range(n):\n",
    "            for j in range(i+1):\n",
    "                Deltas = [np.zeros([n, n])]\n",
    "                Deltas[-1][i, j] = Deltas[-1][j, i] = 1\n",
    "                if i != j:\n",
    "                    Deltas[-1] /= 2\n",
    "                for l in range(depth):\n",
    "                    Deltas.append(\n",
    "                        VBNReLUBack(kers[depth-l-1], Deltas[-1])\n",
    "                    )\n",
    "                for l in range(len(kers)):\n",
    "                    ntk[i, j] += (kers[l].reshape(-1)\n",
    "                                   @ Deltas[-1-l].reshape(-1))\n",
    "                ntk[j, i] = ntk[i, j]\n",
    "        return ntk\n",
    "    \n",
    "    ntk1 = make_single_batch_ntk(ker1s)\n",
    "    ntk2 = make_single_batch_ntk(ker2s)\n",
    "    \n",
    "    ntkx = np.zeros([n1, n2])\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            Deltas = [np.zeros([n1, n2])]\n",
    "            Deltas[-1][i, j] = 1\n",
    "            for l in range(depth):\n",
    "                Deltas.append(\n",
    "                    VBNReLUBackCrossBatch(\n",
    "                        kerxs[depth-l-1], ker1s[depth-l-1], ker2s[depth-l-1],\n",
    "                        Deltas[-1])\n",
    "                )\n",
    "            for l in range(len(kerxs)):\n",
    "                ntkx[i, j] += (kerxs[l].reshape(-1)\n",
    "                               @ Deltas[-1-l].reshape(-1))\n",
    "        \n",
    "    return dict(\n",
    "        ker1s=ker1s,\n",
    "        ker2s=ker2s,\n",
    "        kerxs=kerxs,\n",
    "        ntk1=ntk1,\n",
    "        ntk2=ntk2,\n",
    "        ntkx=ntkx\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Theory vs Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We verify that as width increases, the empirical finite-width NTK converges to the theoretical infinite-width NTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:17.322662Z",
     "start_time": "2020-06-17T21:12:17.224449Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def simkernels(myinp1, myinp2, mynet):\n",
    "    mynet.zero_grad()\n",
    "    mynet.hs = []\n",
    "    out2 = mynet(myinp2)\n",
    "    out2.backward(torch.ones_like(out2))\n",
    "    gg2 = mynet.hs[0].grad.clone()\n",
    "\n",
    "    mynet.zero_grad()\n",
    "    mynet.hs = []\n",
    "    out1 = mynet(myinp1)\n",
    "    out1.backward(torch.ones_like(out1))\n",
    "    gg1 = mynet.hs[0].grad.clone()\n",
    "\n",
    "\n",
    "    hdelta1 = gg1 @ gg1.T # / gg1.shape[1]\n",
    "    hdelta2 = gg2 @ gg2.T # / gg1.shape[1]\n",
    "    hdeltax = gg1 @ gg2.T # / gg1.shape[1]\n",
    "    return [c.detach().numpy() for c in [hdeltax, hdelta1, hdelta2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:17.502486Z",
     "start_time": "2020-06-17T21:12:17.323949Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def simNTK(myinp1, myinp2, mynet):\n",
    "    n1 = myinp1.shape[0]\n",
    "    n2 = myinp2.shape[0]\n",
    "    \n",
    "    if isinstance(myinp1, np.ndarray):\n",
    "        myinp1 = torch.from_numpy(myinp1).float()\n",
    "    if isinstance(myinp2, np.ndarray):\n",
    "        myinp2 = torch.from_numpy(myinp2).float()\n",
    "    \n",
    "    ggs = []\n",
    "    for batch in [myinp1, myinp2]:\n",
    "        gs = []\n",
    "        mynet.zero_grad()\n",
    "        out = mynet(batch)\n",
    "        for i in range(len(batch)):\n",
    "            mynet.zero_grad()\n",
    "            outgrad = torch.zeros_like(out)\n",
    "            outgrad[i] = 1\n",
    "            out.backward(outgrad, retain_graph=True)\n",
    "            g = flatten(clone_grads(mynet))\n",
    "            gs.append(g)\n",
    "        ggs.append(torch.stack(gs))\n",
    "    \n",
    "    ntk1 = ggs[0] @ ggs[0].T\n",
    "    ntk2 = ggs[1] @ ggs[1].T\n",
    "    ntkx = ggs[0] @ ggs[1].T\n",
    "    \n",
    "    return [c.detach().numpy() for c in [ntkx, ntk1, ntk2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:17.627548Z",
     "start_time": "2020-06-17T21:12:17.504003Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def NTK_theory_vs_sim(myinp1, myinp2, \n",
    "                    theorykerx, theoryker1, theoryker2,\n",
    "                    depth,\n",
    "                    mode='ntk',\n",
    "                    log2widthmin=6, log2widthmax=14, nseeds=10):\n",
    "    if isinstance(myinp1, np.ndarray):\n",
    "        myinp1 = torch.from_numpy(myinp1).float()\n",
    "    if isinstance(myinp2, np.ndarray):\n",
    "        myinp2 = torch.from_numpy(myinp2).float()\n",
    "    inputdim = myinp1.shape[-1]\n",
    "    theorynorm1 = np.linalg.norm(theoryker1)\n",
    "    theorynorm2 = np.linalg.norm(theoryker2)\n",
    "    theorynormx = np.linalg.norm(theorykerx)\n",
    "    theorynorm = np.sqrt(theorynorm1**2 + theorynorm2**2 + 2 * theorynormx**2)\n",
    "    frobs = []\n",
    "    widths = 2**np.arange(log2widthmin, log2widthmax)\n",
    "    for width, seed in tqdm(list(product(widths, range(nseeds)))):\n",
    "        torch.manual_seed(seed)\n",
    "        mynet = MyNet(depth=depth, inputdim=inputdim, width=width,\n",
    "                     initstyle=mode.lower())\n",
    "\n",
    "        if mode.lower() == 'ntk':\n",
    "            hdeltax, hdelta1, hdelta2 = simNTK(myinp1, myinp2, mynet)\n",
    "        else:\n",
    "            hdeltax, hdelta1, hdelta2 = simkernels(myinp1, myinp2, mynet)\n",
    "\n",
    "        frob1 = np.linalg.norm(hdelta1 - theoryker1)\n",
    "        frob2 = np.linalg.norm(hdelta2 - theoryker2)\n",
    "        frobx = np.linalg.norm(hdeltax - theorykerx)\n",
    "        frob = np.sqrt(frob1**2 + frob2**2 + 2 * frobx**2)\n",
    "\n",
    "        frobs.append(dict(\n",
    "            width=width,\n",
    "            frob1=frob1,\n",
    "            frob2=frob2,\n",
    "            frobx=frobx,\n",
    "            frob=frob,\n",
    "            relfrob1=frob1/theorynorm1,\n",
    "            relfrob2=frob2/theorynorm2,\n",
    "            relfrobx=frobx/theorynormx,\n",
    "            relfrob=frob/theorynorm,\n",
    "            theorynorm1=theorynorm1,\n",
    "            theorynorm2=theorynorm2,\n",
    "            theorynormx=theorynormx,\n",
    "        ))\n",
    "    return pd.DataFrame(frobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:18.530812Z",
     "start_time": "2020-06-17T21:12:17.629168Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:18.535380Z",
     "start_time": "2020-06-17T21:12:18.532392Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nsamples = 12\n",
    "n1 = 7\n",
    "n2 = 5\n",
    "inpdim = nsamples + 1\n",
    "olddim = 3 * 32 * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:29.173241Z",
     "start_time": "2020-06-17T21:12:18.536680Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "myimages = list(trainset)[:nsamples]\n",
    "myimages = np.array([im[0].numpy() for im in myimages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:29.178826Z",
     "start_time": "2020-06-17T21:12:29.174665Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "myimagesproj = myimages.reshape(nsamples, -1) @ \\\n",
    "            np.random.randn(olddim, inpdim) / np.sqrt(olddim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:29.218845Z",
     "start_time": "2020-06-17T21:12:29.180178Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inker = myimagesproj.reshape(nsamples, -1) @ \\\n",
    "        myimagesproj.reshape(nsamples, -1).T / inpdim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## How much does finite-width NTK deviate from the infinite-width NTK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:29.270472Z",
     "start_time": "2020-06-17T21:12:29.220236Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "depth=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:12:31.687029Z",
     "start_time": "2020-06-17T21:12:29.271733Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inf_kers = theorykernels(\n",
    "    inker[:n1, n1:], inker[:n1, :n1], inker[n1:, n1:], depth=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:14:41.118032Z",
     "start_time": "2020-06-17T21:12:31.691748Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:09<00:00,  6.18it/s]\n"
     ]
    }
   ],
   "source": [
    "frob_df = NTK_theory_vs_sim(\n",
    "    myimagesproj[:n1], myimagesproj[n1:],\n",
    "    inf_kers['ntkx'], inf_kers['ntk1'], inf_kers['ntk2'],\n",
    "    depth=depth,\n",
    "    log2widthmax=14, nseeds=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We measured the relative Frobenius norm of the finite-width deviation = $\\|\\Theta - \\mathring \\Theta\\|_F / \\|\\mathring \\Theta\\|_F$, where $\\Theta$ and $\\mathring \\Theta$ are resp. the finite (empirical) and infinite-width (theoretical) NTKs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:14:42.560347Z",
     "start_time": "2020-06-17T21:14:41.119392Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(data=frob_df, x='width', y='relfrob')\n",
    "widths = frob_df.width.unique()\n",
    "plt.plot(widths, np.array(widths, dtype='float')**-0.5, '--', label=u'${width}^{-1/2}$')\n",
    "plt.ylabel(u'Relative Frob. Norm')\n",
    "plt.loglog()\n",
    "plt.legend()\n",
    "_ = plt.title(u'Batchnorm NTK\\nDeviation from theory drops like $width^{-1/2}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:14:42.566311Z",
     "start_time": "2020-06-17T21:14:42.562151Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "frob_df.to_pickle('batchnorm_ntk.frob')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python36664bitbaseconda707c1567454d4d918345c26606d07f5e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
